{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **ML Pipeline: Predict Sale Price**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a regression model that will predict the Sale Price of a house in Ames, Iowa\n",
        "* Create an ML Pipeline including data cleaning, feature engineering and ML model steps\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePriceRecords.csv\n",
        "* Data Cleaning and Feature Engineering steps found in previous notebooks\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* TrainSet\n",
        "* TestSet\n",
        "* Regression Piepline\n",
        "* Feature importance Plot\n",
        "\n",
        "## CRISP-DM\n",
        "\n",
        "* Modelling\n",
        "* Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The notebooks are stored in a sub folder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(f\"outputs/datasets/collection/HousePriceRecords.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load variables\n",
        "\n",
        "* Below we will declare the relevant variables as decided upon on the Data Cleaning and Feature Engineering notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arbitrary_imputation_vars = ['2ndFlrSF', 'EnclosedPorch', 'WoodDeckSF']\n",
        "median_imputation_vars = ['BedroomAbvGr', 'LotFrontage', 'GarageYrBlt',\n",
        "                          'MasVnrArea']\n",
        "most_frequent_vars = ['BsmtFinType1', 'GarageFinish']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_encoding_vars = (df.select_dtypes(include=['object'])\n",
        "                             .columns.to_list())\n",
        "categorical_encoding_vars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_transformation_vars = ['1stFlrSF', 'LotArea']\n",
        "power_vars = ['2ndFlrSF', 'BsmtFinSF1', 'BsmtUnfSF',\n",
        "              'GarageArea', 'MasVnrArea', 'TotalBsmtSF']\n",
        "yeojohnson_vars = ['GrLivArea']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create ML Pipeline\n",
        "* We create the base for the ML Pipeline\n",
        "* We inlcude the steps for Data Cleaning Feature Engineering, Feature Scaling, Feature Selection and the ML model we decide upon\n",
        "* We will use Hyperparameter Optimization to choose the best model and most effective hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import (MeanMedianImputer,\n",
        "                                       ArbitraryNumberImputer,\n",
        "                                       CategoricalImputer)\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine import transformation as vt\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def PipelineRegressor(model):\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"ArbitraryImputer\",\n",
        "         ArbitraryNumberImputer(arbitrary_number=0,\n",
        "                                variables=arbitrary_imputation_vars)),\n",
        "\n",
        "        (\"MedianImputation\",\n",
        "         MeanMedianImputer(imputation_method='median',\n",
        "                           variables=median_imputation_vars)),\n",
        "\n",
        "        (\"CategoricalImputer\",\n",
        "         CategoricalImputer(imputation_method='frequent',\n",
        "                            variables=most_frequent_vars)),\n",
        "\n",
        "        # Feature Engineering\n",
        "        (\"OrdinalCategoricalEncoder\",\n",
        "         OrdinalEncoder(encoding_method='arbitrary',\n",
        "                        variables=categorical_encoding_vars)),\n",
        "\n",
        "        (\"LogTransformer\", vt.LogTransformer(\n",
        "            variables=['1stFlrSF', 'LotArea'])),\n",
        "\n",
        "        (\"PowerTransformer\", vt.PowerTransformer(variables=power_vars)),\n",
        "\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(\n",
        "            variables=yeojohnson_vars)),\n",
        "\n",
        "        (\"Winsorizer\", Winsorizer(capping_method='iqr', tail='both', fold=1.5,\n",
        "                                  variables=['GrLivArea'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\",\n",
        "         SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                  threshold=0.75,\n",
        "                                  selection_method=\"variance\")),\n",
        "\n",
        "        # Feature Scaling\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # Feature Selection\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "\n",
        "        # ML Algorithms\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Hyperparameter Optimization Search**\n",
        "    * Below code taken from CI lesson: *Scikit-Learn Unit 6: Cross Validation Search Part 2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineRegressor(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score',\n",
        "                   'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Split Train & Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                    df.drop(['SalePrice'], axis=1),\n",
        "                                    df['SalePrice'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0,\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We import our potential Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikit Learn Cross Validation Search\n",
        "\n",
        "* We first search for the most suitable algorithm using tandard hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
        "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"LogisticRegression\": {},\n",
        "    \"LinearRegression\": {},\n",
        "    \"GradientBoostingRegressor\": {},\n",
        "    \"XGBRegressor\": {},\n",
        "    \"DecisionTreeRegressor\": {},\n",
        "    \"RandomForestRegressor\": {},\n",
        "    \"GradientBoostingClassifier\": {},\n",
        "    \"ExtraTreesRegressor\": {},\n",
        "    \"AdaBoostRegressor\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We will include LogisticRegression in our first run. For this the max cross validation value we can have is cv=2.\n",
        "* We will check how LogisticRegression performs against the other algorithms. If it is not the best performer, we will remove it from our search and run again with a greater cross validation value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, r2_score\n",
        "\n",
        "search = HyperparameterOptimizationSearch(models=models_quick_search,\n",
        "                                          params=params_quick_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We add the results to a DataFrame and assess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "grid_search_summary,\n",
        "grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "print(grid_search_summary.shape)\n",
        "grid_search_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* LogisticRegression had the worst overall mean R2 score so we can remove it and run again with corss valdation value of cv=5\n",
        "* We redefine our variables without LogisticRegression and complete another search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
        "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"LinearRegression\": {},\n",
        "    \"GradientBoostingRegressor\": {},\n",
        "    \"XGBRegressor\": {},\n",
        "    \"DecisionTreeRegressor\": {},\n",
        "    \"RandomForestRegressor\": {},\n",
        "    \"GradientBoostingClassifier\": {},\n",
        "    \"ExtraTreesRegressor\": {},\n",
        "    \"AdaBoostRegressor\": {},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_quick_search,\n",
        "                                          params=params_quick_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We check how the algorithms performed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary,\n",
        "grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "print(grid_search_summary.shape)\n",
        "grid_search_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* LinearRegression provided us with our best mean score of 0.78.\n",
        "* This score is sufficient to meet the performance goal of our model as per our client's business case (R2 score > 0.75)\n",
        "* We can see that GradientBoostingRegressor also provides us with a suffiecient R2 score so we may consider this also \n",
        "* Next we will try find the best hyperparameters for our models and try to fine tune them so that we can improve the scores on our TrainSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
        "}\n",
        "\n",
        "# }\n",
        "# params_search = {\n",
        "#     \"ExtraTreesRegressor\": {\n",
        "#         'model__max_depth': [3,5,10],\n",
        "#         'model__min_impurity_decrease': [0.0,0.3,0.5,1],\n",
        "#         'model__min_samples_leaf': [0.1,0.5,1],\n",
        "#         'model__min_samples_split': [2,3,5],\n",
        "#         'model__min_weight_fraction_leaf': [0.0,0.1,0.5],\n",
        "#         'model__n_estimators': [100,300,400]\n",
        "#     }\n",
        "# }\n",
        "params_search = {\n",
        "    \"ExtraTreesRegressor\": {\n",
        "        'model__max_depth': [10],\n",
        "        'model__min_impurity_decrease': [0.0],\n",
        "        'model__min_samples_leaf': [1],\n",
        "        'model__min_samples_split': [5],\n",
        "        'model__min_weight_fraction_leaf': [0.0],\n",
        "        'model__n_estimators': [100, 300]}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We perform another Cross Validation search using \"LinearRegression\" & \"GradientBoostingRegressor\" as our model and a set of hyperparameters to apply, in order to find the best hyperparameter permutation for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_search,\n",
        "                                          params=params_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We add the results to a DataFrame and assess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary,\n",
        "grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "grid_search_summary.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We can see that we have an improvement in the mean R2 score, bringing it up to 0.81\n",
        "* We will use these hyperparameters for our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We print our best model below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We print our best combination of hyperparameters for our algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_hyperparams = grid_search_pipelines[best_model].best_params_\n",
        "best_hyperparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We define our best pipeline at this stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_regressor = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Measure performance on Train & Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "\n",
        "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
        "    print(\"Model Evaluation \\n\")\n",
        "    print(\"* Train Set\")\n",
        "    regression_evaluation(X_train, y_train, pipeline)\n",
        "    print(\"* Test Set\")\n",
        "    regression_evaluation(X_test, y_test, pipeline)\n",
        "\n",
        "\n",
        "def regression_evaluation(X, y, pipeline):\n",
        "    prediction = pipeline.predict(X)\n",
        "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
        "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
        "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
        "    print('Root Mean Squared Error:',\n",
        "          np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline,\n",
        "                                alpha_scatter=0.5):\n",
        "    pred_train = pipeline.predict(X_train)\n",
        "    pred_test = pipeline.predict(X_test)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 12))\n",
        "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
        "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
        "    axes[0].set_xlabel(\"Actual\")\n",
        "    axes[0].set_ylabel(\"Predictions\")\n",
        "    axes[0].set_title(\"Train Set\")\n",
        "\n",
        "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
        "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
        "    axes[1].set_xlabel(\"Actual\")\n",
        "    axes[1].set_ylabel(\"Predictions\")\n",
        "    axes[1].set_title(\"Test Set\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regression_performance(X_train, y_train, X_test, y_test, pipeline_regressor)\n",
        "regression_evaluation_plots(X_train, y_train, X_test, y_test,\n",
        "                            pipeline_regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The R2 score on our test set is 0.782.\n",
        "* This is an acceptable score, in the context of our business case, for our house price prediction model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assess feature importance\n",
        "\n",
        "* We are now interetsed to check which are the most important features for our pipeline and as our algorithm is Tree Based, we can access these using the 'feature_importances_' method.\n",
        "    * The below code was taken from CI walthrough project: *Churnometer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# number of data cleaning and feature engineering in pipeline\n",
        "data_cleaning_feat_eng_steps = 9\n",
        "columns_after_data_cleaning_feat_eng = ((Pipeline(pipeline_regressor\n",
        "                                        .steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns))\n",
        "\n",
        "best_features = (columns_after_data_cleaning_feat_eng[pipeline_regressor\n",
        "                 ['feat_selection'].get_support()].to_list())\n",
        "\n",
        "# # create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "          'Feature': (columns_after_data_cleaning_feat_eng\n",
        "                      [pipeline_regressor['feat_selection'].get_support()]),\n",
        "          'Importance': pipeline_regressor['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "# # Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in\" +\n",
        "      f\"descending order. \"\n",
        "      f\"The model was trained on them:\" +\n",
        "      f\"\\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The most important features for predicting 'SalePrice' will be printed out below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rewrite & Refit Pipeline with best features\n",
        "* We rewrite our pipeline removing all steps whihc do not include our most important features.\n",
        "* We also remove SmartCorelatedSelectio and Feature Selection. As we have our most important features, these are no longer necessary.\n",
        "* We aim to streamline our pipeline to predict 'SalePrice' using fewer features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineRegressor(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"PowerTransformer\", vt.PowerTransformer(variables=['GarageArea',\n",
        "                                                            'TotalBsmtSF'])),\n",
        "\n",
        "        (\"YeoJohnsonTransformer\",\n",
        "         vt.YeoJohnsonTransformer(variables=yeojohnson_vars)),\n",
        "\n",
        "        (\"Winsorizer\", Winsorizer(capping_method='iqr', tail='both', fold=1.5,\n",
        "                                  variables=['GrLivArea'])),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating on Train and Test Sets\n",
        "\n",
        "* We split our Train & Test Sets again using only our most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                    df.drop(['SalePrice'], axis=1),\n",
        "                                    df['SalePrice'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",\n",
        "      X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.filter(best_features)\n",
        "X_test = X_test.filter(best_features)\n",
        "\n",
        "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",\n",
        "      X_test.shape, y_test.shape)\n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We define our highest performing model and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0)\n",
        "}\n",
        "\n",
        "params_search = {\n",
        "    \"ExtraTreesRegressor\": {\n",
        "        'model__max_depth': [10],\n",
        "        'model__min_impurity_decrease': [0.0],\n",
        "        'model__min_samples_leaf': [1],\n",
        "        'model__min_samples_split': [5],\n",
        "        'model__min_weight_fraction_leaf': [0.0],\n",
        "        'model__n_estimators': [100]}\n",
        "\n",
        "}\n",
        "\n",
        "params_search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We assess the performance on the scaled down TrainSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_search,\n",
        "                                          params=params_search)\n",
        "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary,\n",
        "grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "grid_search_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Our R2 score remains at 0.81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We define our best model and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_regressor = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# number of data cleaning and feature engineering in pipeline\n",
        "data_cleaning_feat_eng_steps = 3\n",
        "columns_after_data_cleaning_feat_eng = ((Pipeline(pipeline_regressor\n",
        "                                        .steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns))\n",
        "\n",
        "best_features = (columns_after_data_cleaning_feat_eng[pipeline_regressor\n",
        "                 ['feat_selection'].get_support()].to_list())\n",
        "\n",
        "# # create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "          'Feature': (columns_after_data_cleaning_feat_eng\n",
        "                      [pipeline_regressor['feat_selection'].get_support()]),\n",
        "          'Importance': pipeline_regressor['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "# # Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in\" +\n",
        "      f\"descending order. \"\n",
        "      f\"The model was trained on them:\" +\n",
        "      f\"\\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The feature importance remains the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We assess the performance on our TestSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regression_performance(X_train, y_train, X_test, y_test, pipeline_regressor)\n",
        "regression_evaluation_plots(X_train, y_train, X_test, y_test,\n",
        "                            pipeline_regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The R2 score remains at 0.782."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo\n",
        "\n",
        "We will generate the following files for our dashboard:\n",
        "\n",
        "* Train set\n",
        "* Test set\n",
        "* Modelling Pipeline\n",
        "* Fetaure Importance Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v2'\n",
        "file_path = f\"outputs/ml_pipeline/predict_saleprice/{version}\"\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelling Pipeline\n",
        "\n",
        "* ML pipeline for SalePrice prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_regressor,\n",
        "            filename=f\"{file_path}/pipeline_regressor.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x=\"Feature\", y=\"Importance\")\n",
        "plt.savefig(f\"{file_path}/features_importance.png\", bbox_inches=\"tight\",\n",
        "            facecolor='white')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
