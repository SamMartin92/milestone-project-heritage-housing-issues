{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* We want to select the best feature engineering methods to be used in our pipeline\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/cleaned/TrainSetCleaned.csv\n",
        "* outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* We will have lists of variables and how we should engineer each for the purposes of our pipeline.\n",
        "\n",
        "## CRISP-DM\n",
        "\n",
        "* Data preparation \n",
        "\n",
        "## Conclusions\n",
        "\n",
        "* We will take the following actions:\n",
        "    * Log_e transformation: '1stFlrSF', 'LotArea'\n",
        "    * Power transformation: 'BsmtFinSF1', 'BsmtUnfSF', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF', 'GarageArea'\n",
        "    * Yeo Johnson transformation: 'GarageArea', 'GrLivArea'\n",
        "    * Outlier Winsorizer: 'GrLivArea'\n",
        "    * Smart Correlated Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The notebooks are stored in a sub folder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We see from this report that a number of our variable have skewed distributions. We will attempt to normalise some of these distributions with Numerical Transformations\n",
        "* There are ome variables with outliers, we may use Winorizer of Outlier trimmer to handle these anomalies in the data\n",
        "* We will use Categorical Variable Encoding on our categorical varaibles in order to convert them to integers, which our model will be able to handle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation and PPS analysis\n",
        "\n",
        "* We will run a correlation and power predictive score analysis to investigate the correlation levels between variables in our TrainSet\n",
        "    * Custom functions used are taken & adapted from *CI Walkthrough Project: Churnometer Data Cleaning notebook*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "sns.set(style=\"whitegrid\")\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "    fig, axes = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                mask=mask, cmap='viridis', annot_kws={\"size\": font_annot},\n",
        "                ax=axes,\n",
        "                linewidth=0.5\n",
        "                )\n",
        "    axes.set_yticklabels(df.columns, rotation=0)\n",
        "    plt.ylim(len(df.columns), 0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r',\n",
        "                         annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = (pps_matrix_raw.filter(['x', 'y', 'ppscore'])\n",
        "                  .pivot(columns='x', index='y', values='ppscore')\n",
        "                  )\n",
        "\n",
        "    pps_score_stats = (pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore'])\n",
        "                       .describe().T)\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for\" +\n",
        "          \"heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,\n",
        "                      CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize,\n",
        "                 font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous\" +\n",
        "          \"variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold,\n",
        "                 figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two\" +\n",
        "          \"columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect\" +\n",
        "          \"predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize,\n",
        "                font_annot=font_annot)\n",
        "\n",
        "\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(TrainSet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The most common PPS scores are between 0 and 0.75\n",
        "* We now plot heatmaps in order to check levels of correlation between variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
        "                  df_corr_spearman=df_corr_spearman,\n",
        "                  pps_matrix=pps_matrix,\n",
        "                  CorrThreshold=0.4, PPS_Threshold=0.2,\n",
        "                  figsize=(12, 10), font_annot=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We note there is a relatively high correlation bewteen SalesPrice and a number of variables\n",
        "* For PPS, we note that only 'KitchenQual' & 'OverallQual' have relatively high predictive power for 'SalesPrice', whereas a number of other variables have high PPS between themselves.\n",
        "* It seems that it will be necessary to remove some surplus correlated features in order to not overfit the model. We will use SmartCorrelatedSelection to handle this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "\n",
        "* As stated above, during our data exploration, we will look at the following techniques to transform our data:\n",
        "    1. Numerical Transformation\n",
        "    2. Handle outliers\n",
        "    3. Categorical Encoding\n",
        "    4. Smart Correlated Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The below functions will help with our feture engineering analysis and allow us to choose how to proceed\n",
        "    * Custom functions taken from CI lesson: *Feature Engine Unit 9: Custom Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "import scipy.stats as stats\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like pandas-profiling, to\n",
        "    evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop over each variable and engineer the data according to\n",
        "    # the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "    # Apply transformers in respectives column_transformers\n",
        "    df_feat_eng, list_applied_transformers = apply_transformers(analysis_type,\n",
        "                                                                df_feat_eng,\n",
        "                                                                column)\n",
        "\n",
        "    # For each variable, assess how the transformations perform\n",
        "    transformer_evaluation(column, list_applied_transformers, analysis_type,\n",
        "                           df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    # Check analyis type\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(f\"You should pass analysis_type parameter as one of\" +\n",
        "                         \" the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(f\"analysis_type argument should be one of these \" +\n",
        "                         \"options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "                        f\"There is missing values in your dataset. Please \" +\n",
        "                        \"handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    # Set suffix colummns acording to analysis_type\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\"log_e\", \"log_10\", \"reciprocal\", \"power\",\n",
        "                                    \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng,\n",
        "        list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,\n",
        "                                                              column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers =\n",
        "        FeatEngineering_OutlierWinsorizer(df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers =\n",
        "        FeatEngineering_CategoricalEncoder(df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type,\n",
        "                           df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "    else:\n",
        "        if col == column:\n",
        "            DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "        else:\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=['#432371'],\n",
        "                  order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                 variables=[f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "                        capping_method='iqr', tail='both', fold=1.5,\n",
        "                        variables=[f\"{column}_iqr\"]\n",
        "                        )\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Encoding\n",
        "* We select variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_encoding_vars = (TrainSet.select_dtypes(include=['object']).\n",
        "                             columns.to_list())\n",
        "categorical_encoding_vars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create a separate data frame with the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = TrainSet[categorical_encoding_vars].copy()\n",
        "df_engineered.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create our engineered variables and assess the distribution & values.\n",
        "* In this case, we will try ordinal encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = FeatureEngineeringAnalysis(df=df_engineered,\n",
        "                                           analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* With this transformation, the distribution does not change for any of the variable.\n",
        "* We can consider this a successful transformation as the categories have all changed to integers.\n",
        "* We will apply this transformation, as is, as part of our feature engineering. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc = OrdinalEncoder(encoding_method='arbitrary',\n",
        "                     variables=categorical_encoding_vars)\n",
        "TrainSet = enc.fit_transform(TrainSet)\n",
        "TestSet = enc.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We confirm the transformations on TrainSet & TestSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.filter(categorical_encoding_vars).head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.filter(categorical_encoding_vars).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numerical transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We select the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_transformation_vars = (TrainSet.\n",
        "                                 select_dtypes(include=['float', 'int'])\n",
        "                                 .columns.to_list())\n",
        "numerical_transformation_vars.remove('SalePrice')\n",
        "numerical_transformation_vars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create a separate DataFrame with the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = TrainSet[numerical_transformation_vars].copy()\n",
        "df_engineered.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create our engineered variables and assess the distribution & values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = FeatureEngineeringAnalysis(df=df_engineered,\n",
        "                                           analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upon analysis of the numerical transformtions, we will take the following actions:\n",
        "\n",
        "* For '1stFlrSF' & 'LotArea', log_e seems to have the most positive effect on distribution and weill apply it to these variables.\n",
        "\n",
        "* For 2ndFlrSF', 'BsmtFinSF1', 'BsmtUnfSF', 'GarageArea', 'MasVnrArea','TotalBsmtSF' we will apply Power transfomrer which imporved the distribution of these variables slightly.\n",
        "\n",
        "* For 'GrLivArea', yeo johnson shows the most positive effect on our distribution and we will apply it here.\n",
        "\n",
        "* We will leave the rest of the variables as they are for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_transformation_vars = ['1stFlrSF', 'LotArea']\n",
        "log_transformation_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = vt.LogTransformer(variables=log_transformation_vars)\n",
        "TrainfSet = tf.fit_transform(TrainSet)\n",
        "TestSet = tf.transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "power_vars = ['BsmtFinSF1', 'BsmtUnfSF', 'MasVnrArea', 'OpenPorchSF',\n",
        "              'TotalBsmtSF', 'GarageArea']\n",
        "power_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = vt.PowerTransformer(variables=power_vars)\n",
        "TrainfSet = tf.fit_transform(TrainSet)\n",
        "TestSet = tf.transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeo_johnson_vars = ['GarageArea', 'GrLivArea']\n",
        "yeo_johnson_vars "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = vt.YeoJohnsonTransformer(variables=yeo_johnson_vars)\n",
        "TrainfSet = tf.fit_transform(TrainSet)\n",
        "TestSet = tf.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We ensure the numeric transformations take effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_report = ProfileReport(df=TrainSet.filter(log_transformation_vars +\n",
        "                                                 power_vars +\n",
        "                                                 yeo_johnson_vars),\n",
        "                              minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handle Outliers\n",
        "\n",
        "* We can see that a number of our variables have outliers.\n",
        "\n",
        "* We will assess the affects using Winsorizer as an outlier trimmer will have on the engineered variables and decide how we wish to handle outliers in our variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = TrainSet[numerical_transformation_vars].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = FeatureEngineeringAnalysis(df=df_engineered,\n",
        "                                           analysis_type='outlier_winsorizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* While the Winsorizer does seem to normalise some of the distributions in our dataset, these outliers appear to reflect the varied properties in our dataset rather than corrupt or incorrect data so we may not need to apply this step to our pipeline.\n",
        "\n",
        "* For 'GrLivArea' however, it does has a very positive effect on the data so we can apply the transformer for this variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "winz = Winsorizer(capping_method='iqr', tail='both', fold=1.5, \n",
        "                    variables=['GrLivArea'])\n",
        "TrainSet = winz.fit_transform(TrainSet)\n",
        "TestSet=winz.transform(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmartCorrelatedSelection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We will run smart correlated selction on all of our variables apart from our target which is 'SalePrice. We will use the pearson method as our final ML model is likely to be a regression model. We can revisit this stage if our model is inefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smart_correlation_features = TrainSet.columns.to_list() \n",
        "smart_correlation_features.pop()\n",
        "smart_correlation_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create a separate dataframe with our variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineered = TrainSet.copy()\n",
        "df_engineered.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We create our engineered variables and analyse which variables to drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=smart_correlation_features,\n",
        "                                    method=\"spearman\", threshold=0.75,\n",
        "                                    selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineered)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* '1stFlrSF' & 'GarageYrBlt' will be dropped in our pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Next steps:\n",
        "\n",
        "* We will add the following transformations to our ML Pipeline:\n",
        "    * Log_e transformation to '1stFlrSF', 'LotArea'\n",
        "    * Power transformation to 'BsmtFinSF1', 'BsmtUnfSF', 'MasVnrArea', 'OpenPorchSF', 'TotalBsmtSF', 'GarageArea'\n",
        "    * Yeo Johnson transformation to 'GarageArea', 'GrLivArea'\n",
        "    * Apply Winsorizer to 'GrLivArea' after Yeo Johnson transformation\n",
        "    * Run Smart Correlated Selection on all variables\n",
        "        * '1stFlrSF' & 'GarageYrBlt' to be dropped in our pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In the next notebook, we will build our ML Pipeline and conduct modelling, choosing the best algorithms and hyperparamters to fit our model.\n",
        "* There are no files to push to the repo in this notebook."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Oct 14 2022, 17:27:18) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
